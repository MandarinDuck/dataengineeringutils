{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[boto docs](http://boto3.readthedocs.io/en/latest/reference/services/glue.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.environ['PATH'])\n",
    "print(os.environ['JAVA_HOME'])\n",
    "print(os.environ['CLASSPATH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook attempts to create a database in glue and a crawler that interatively updates the database with new data from an S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the packages I need\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import boto3\n",
    "from io import StringIO\n",
    "from pyathenajdbc import connect\n",
    "import data_engineering_utils.glue as glue_utils\n",
    "import data_engineering_utils.meta as meta_utils\n",
    "import data_engineering_utils.basic as basic_utils\n",
    "import copy\n",
    "import datetime as dt\n",
    "\n",
    "staging_bucket = 'alpha-karik-glue-tests'\n",
    "test_data_folder = '/Users/karik/Documents/projects/test_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep data\n",
    "# For each dataset we need to create a table.meta.json file for each table\n",
    "# In this example I just set everything to default (all cols are strings with a default description)\n",
    "name_list = ['department', 'employees', 'sales']\n",
    "for name in name_list :\n",
    "    d = pd.read_csv(test_data_folder + name + '.csv')\n",
    "    cols = meta_utils.get_col_types_from_df(d)\n",
    "    table_meta = meta_utils.Table_metadata(table_name = name,\n",
    "                                           bucket = staging_bucket,\n",
    "                                           columns = cols,\n",
    "                                           table_description = name + ' table description')\n",
    "    basic_utils.write_json(table_meta, test_data_folder + 'meta/' + name + '.meta.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an example can also do this (we'll improve the meta data for sales):\n",
    "sales_cols = basic_utils.get_csv_header(file_path = (test_data_folder + 'sales.csv'),\n",
    "                            convert_to_lower = True)\n",
    "\n",
    "cols = meta_utils.create_columns_meta(column_names = sales_cols,\n",
    "                                      data_type_overrides = {'qtr' : 'bigint',\n",
    "                                                            'sales' : 'double'},\n",
    "                                      description_overrides = {'staff_id' : 'id of sales staff',\n",
    "                                                               'qtr' : 'quarter at which sales were made',\n",
    "                                                               'sales' : 'total sales for that quarter'})\n",
    "table_meta = meta_utils.Table_metadata(table_name = 'sales',\n",
    "                                       bucket = staging_bucket,\n",
    "                                       columns = cols,\n",
    "                                       table_description = 'Table containing sales totals for each quarter')\n",
    "basic_utils.write_json(table_meta, test_data_folder + 'meta/sales.meta.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have the meta data and csvs for each table we can send them to S3\n",
    "# This should work but doesn't cos of permissions\n",
    "db_name = 'my_first_database'\n",
    "glue_utils.create_database(db_description= 'Some test database', db_name = db_name)\n",
    "\n",
    "# Read in test data and pump it into S3 buckets and then add to my db\n",
    "name_list = ['department', 'employees', 'sales']\n",
    "for name in name_list :\n",
    "    # Read data and meta\n",
    "    d = pd.read_csv(test_data_folder + name + '.csv')\n",
    "    m = basic_utils.read_json(test_data_folder + 'meta/' + name + '.meta.json')\n",
    "    \n",
    "    # Get the table spec for glue\n",
    "    table_spec = glue_utils.get_glue_table_spec_from_meta(meta_object = m, template_type = 'csv')\n",
    "\n",
    "    # Save df in S3 bucket\n",
    "    path = staging_bucket + '/' + name + '/' + dt.datetime.today().strftime(\"%Y_%m_%d\") + '.csv'\n",
    "    glue_utils.df_to_csv_s3(df = d, bucket = staging_bucket, path = path)\n",
    "    \n",
    "    # Create glue table\n",
    "    glue_utils.create_table_from_def(db_name = db_name, table_name = name, table_spec = table_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_path_to_athena_out = \"s3://\" + staging_bucket + '/athena_outputs'\n",
    "conn = connect(s3_staging_dir=s3_path_to_athena_out,\n",
    "               region_name='eu-west-1')\n",
    "sql = \"\"\"\n",
    "select * from alpha-karik-glue-tests.department\n",
    "\"\"\"\n",
    "pd.read_sql(sql, conn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
